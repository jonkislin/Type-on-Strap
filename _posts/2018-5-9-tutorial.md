---
layout: post
title: "Tutorial Highlight: 3Blue1Brown"
published: true
tags: "linear_algebra"
---

To keep this blog-train going, and prove to myself that I'm being
productive while I'm not sending out applications and networking, I'd highlight some
of the awesome tutorials I spend my mornings watching and taking notes on, and discuss some of the things I learned along the way. At worst, this will be a reminder to myself of my own progress, and at best,
perhaps someone will see this post and get to see what I've seen.  

So here we go: [3blue1Brown](http://www.3blue1brown.com).  

What a refreshing video series! Every video so far, from
Fourier Transforms to basic Linear Algebra to Neural Networks has been crystal clear,
well-paced, and engaging. No surprise, as the creator,
[Grant Sanderson](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/about),
worked for [Khan Academy](https://www.khanacademy.org).

### Linear Algebra basics

I'll make no attempt at hiding it: Linear Algebra hasn't been my strongest area, even
after the bootcamp, so I've been making an effort to address some gaps in my
understandings of the basics.

For example, until today, I really don't think I fully understood why a matrix would be considered a [linear transformation](https://www.youtube.com/watch?v=kYB8IZa5AuE&vl=en), or why a vector could be conceived by a physicist
as a direction coupled with a magnitude (such as velocity) and simultaneously as a list of numbers in either a column or a row by a mathematician. The thoughtful but simple animations in this video tie the latter ideas together beautifully:

<br>
<iframe width="1000" height="380" src="https://www.youtube-nocookie.com/embed/fNk_zzaMoSs?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<br>  


Here's the video (also linked above) on matrices as Linear Transformations:

<br>
<iframe width="1000" height="380" src="https://www.youtube-nocookie.com/embed/kYB8IZa5AuE?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<br>

### Neural Networks

In the final weeks of the data science bootcamp at [Metis](https://thisismetis.com),
I was given the opportunity to progress with Deep Learning or with a topic and project
of my own choosing. I chose the latter (Social Network Analysis - see my post in progress on [Traversing the Trump Twitterverse](/2018/05/03/traverse.html).

Given that the bootcamp is long over, and all the buzz I hear about Deep Learning,
I decided to dive into the basics. Next week, I hope to start playing around with MNist dataset. But, for this week, here are some videos that, like with the Linear Algebra basics, have brought clarity to something that I've not been able to fully understand:
<br>
#### Part 1: But what *is* a Neural Network?
<iframe width="1000" height="380" src="https://www.youtube-nocookie.com/embed/aircAruvnKk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<br>

#### Part 2: Gradient Descent
<iframe width="1000" height="380" src="https://www.youtube-nocookie.com/embed/IHZwWFHWa-w?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<br>

#### Part 3: Backpropagation
<iframe width="1000" height="380" src="https://www.youtube-nocookie.com/embed/Ilg3gGewQ5U?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<br>

It was actually these videos on Neural Networks that got me thinking about Linear Algebra again.   
Until next time, thanks for reading!
